{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 · Minimal Active Inference on a Ring World\n",
    "\n",
    "This notebook demonstrates a compact, from-scratch **active inference** agent on a discrete ring world.\n",
    "\n",
    "**Key equations**\n",
    "- **Posterior (filter):** $q(s) \\propto A[o,s]\\, (B[a]q)$\n",
    "- **EFE (risk + ambiguity):** $G = D_{\\mathrm{KL}}(Q(o)\\Vert P(o)) + \\mathbb{E}_{Q(s')}{[H(P(o\\mid s'))]}$\n",
    "- **EFE (cost − information gain):** $G = \\mathbb{E}_{Q(o)}[-\\ln P(o)] - I(S';O)$\n",
    "\n",
    "We compute both decompositions and verify their numerical equivalence (up to constants) while rolling the agent forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from persystems.gm import GenerativeModel, softmax\n",
    "from persystems.inference import bayes_predict, bayes_update, belief_entropy\n",
    "from persystems.planning import choose_action\n",
    "from persystems.viz import plot_efe_trace, plot_cost_info, plot_entropy, plot_final_posterior\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the generative model $(A,B,C)$\n",
    "- $A[o,s]=P(o\\mid s)$ (likelihood)\n",
    "- $B[a][s',s]=P(s'\\mid s,a)$ (per-action transition)\n",
    "- $C$ are log-preferences; $P(o)=\\operatorname{softmax}(C)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "target_idx = 3\n",
    "gm = GenerativeModel.make_ring_world(N=N, A_eps=0.15, target_idx=target_idx)\n",
    "print('Actions:', gm.actions)\n",
    "print('Preferred outcomes P(o):', softmax(gm.C))\n",
    "print('A shape', gm.A.shape, 'B len', len(gm.B), 'C shape', gm.C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roll-out with horizon $H=1$ (myopic EFE)\n",
    "At each step we:\n",
    "1. Evaluate $G$ for each candidate action.\n",
    "2. Take the action with minimal $G$.\n",
    "3. Observe $o_t\\sim P(o\\mid s_t)$.\n",
    "4. Update $q(s_t\\mid o_{1:t})$ by Bayes (filter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 30\n",
    "qs = np.ones(N)/N                    # initial belief over states\n",
    "true_s = np.random.randint(0, N)    # hidden true state\n",
    "\n",
    "hist = {k: [] for k in ['t','true_s','obs','a','G','risk','ambiguity','exp_cost','info_gain','Hq']}\n",
    "for t in range(T):\n",
    "    a_idx, comp, Gs = choose_action(qs, gm.A, gm.B, gm.C, horizon=1)\n",
    "    a = gm.actions[a_idx]\n",
    "    true_s = (true_s + a) % N\n",
    "    o = int(np.random.choice(np.arange(N), p=gm.A[:, true_s]))\n",
    "    q_pred = bayes_predict(qs, gm.B[a_idx])\n",
    "    qs = bayes_update(q_pred, gm.A, o)\n",
    "    # log\n",
    "    hist['t'].append(t)\n",
    "    hist['true_s'].append(true_s)\n",
    "    hist['obs'].append(o)\n",
    "    hist['a'].append(a)\n",
    "    hist['G'].append(float(Gs[a_idx]))\n",
    "    hist['risk'].append(float(comp['risk']))\n",
    "    hist['ambiguity'].append(float(comp['ambiguity']))\n",
    "    hist['exp_cost'].append(float(comp['expected_cost']))\n",
    "    hist['info_gain'].append(float(comp['info_gain']))\n",
    "    hist['Hq'].append(float(belief_entropy(qs)))\n",
    "\n",
    "for k in hist: hist[k] = np.array(hist[k])\n",
    "df = pd.DataFrame(hist)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots: two EFE decompositions + belief entropy and final posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_efe_trace(hist['t'], hist['G'], hist['risk'], hist['ambiguity'])\n",
    "plot_cost_info(hist['t'], hist['exp_cost'], hist['info_gain'])\n",
    "plot_entropy(hist['t'], hist['Hq'])\n",
    "plot_final_posterior(qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth-$H$ planning (tiny lookahead)\n",
    "Enumeration over observation branches scales as $(|\\mathcal{A}|\\,|\\mathcal{O}|)^H$, so keep $H$ small in discrete demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for H in [1, 2]:\n",
    "    qs0 = np.ones(N)/N\n",
    "    a_idx, comp, Gs = choose_action(qs0, gm.A, gm.B, gm.C, horizon=H)\n",
    "    print(f\"H={H} → best action index {a_idx}, EFE per action {np.round(Gs, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- The **risk** term pulls outcomes toward preferences $P(o)$.\n",
    "- The **ambiguity** (or the negative **information gain**) drives epistemic behavior.\n",
    "- Depth-$H$ planning quickly becomes the computational bottleneck; pruning and amortization help (see future notebooks)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

