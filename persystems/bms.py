"""
bms.py — Bayesian model selection utilities for discrete generative models.

Provides:
- log_evidence_discrete(...): exact marginal log-evidence under a GM {A,B} given (actions, observations)
- compare_models_ffx(...): fixed-effects model comparison (sums log-evidence over sequences)
- compare_models_rfx(...): random-effects BMS (Dirichlet posterior over model frequencies, exceedance probs)
"""
from __future__ import annotations
import numpy as np
from typing import List, Sequence, Dict, Tuple
from .gm import softmax
from .inference import bayes_predict, bayes_update


def log_evidence_discrete(
    A: np.ndarray,
    B: List[np.ndarray],
    actions: Sequence[int],
    observations: Sequence[int],
    q0: np.ndarray | None = None
) -> float:
    """
    Exact marginal log-evidence p(o_{1:T} | model) via forward filtering.
    Assumes actions are known (agent- or experimenter-chosen).
    """
    O, S = A.shape
    assert len(B) > 0 and B[0].shape == (S, S)
    T = len(observations)
    if q0 is None:
        q = np.ones(S) / S
    else:
        q = np.asarray(q0, dtype=float).copy()
        q /= q.sum()

    log_ev = 0.0
    for t in range(T):
        a_idx = actions[t]
        q_pred = bayes_predict(q, B[a_idx])
        Qo = A @ q_pred
        o = observations[t]
        p_ot = float(max(Qo[o], 1e-18))
        log_ev += np.log(p_ot)
        q = bayes_update(q_pred, A, o)
    return float(log_ev)


def compare_models_ffx(
    models: List[Dict[str, object]],
    sequences: List[Tuple[Sequence[int], Sequence[int]]],
    q0: np.ndarray | None = None,
    log_prior: np.ndarray | None = None
) -> Dict[str, np.ndarray]:
    """
    Fixed-effects BMS: sum log-evidence across sequences per model and return posterior over models.
    models: list of dicts with keys {'A','B','name'}
    sequences: list of (actions, observations)
    """
    M = len(models)
    le = np.zeros(M)
    for m_idx, m in enumerate(models):
        total = 0.0
        for (acts, obs) in sequences:
            total += log_evidence_discrete(m["A"], m["B"], acts, obs, q0=q0)
        le[m_idx] = total
    if log_prior is None:
        log_prior = np.zeros(M)
    post = np.exp((le + log_prior) - np.max(le + log_prior))
    post /= post.sum()
    return {"log_evidence": le, "post": post}


def compare_models_rfx(
    models: List[Dict[str, object]],
    sequences: List[Tuple[Sequence[int], Sequence[int]]],
    alpha0: float = 1.0,
    nsamples: int = 20000
) -> Dict[str, np.ndarray]:
    """
    Random-effects BMS: treat each sequence as generated by one of the models;
    return posterior over model frequencies r ~ Dir(alpha), plus exceedance probabilities.

    Implementation note:
    - We compute per-sequence model posterior responsibilities via softmax of log-evidence.
    - Dirichlet posterior α = α0 + sum responsibilities across sequences.
    - Exceedance probs via sampling r ~ Dir(α) and counting argmax.
    """
    M = len(models)
    S = len(sequences)
    # per-sequence log evidence matrix
    logE = np.zeros((S, M))
    for s_idx, (acts, obs) in enumerate(sequences):
        for m_idx, m in enumerate(models):
            logE[s_idx, m_idx] = log_evidence_discrete(m["A"], m["B"], acts, obs, q0=None)
    # responsibilities
    R = np.exp(logE - logE.max(axis=1, keepdims=True))
    R /= R.sum(axis=1, keepdims=True)
    alpha = alpha0 + R.sum(axis=0)

    # exceedance by Dirichlet sampling
    rng = np.random.default_rng(0)
    samples = rng.gamma(alpha, 1.0, size=(nsamples, M))
    samples /= samples.sum(axis=1, keepdims=True)
    ex = np.zeros(M)
    winners = samples.argmax(axis=1)
    for m in range(M):
        ex[m] = np.mean(winners == m)

    # group mean frequencies
    r_mean = alpha / alpha.sum()
    return {"alpha": alpha, "responsibilities": R, "exceedance": ex, "r_mean": r_mean, "logE": logE}

